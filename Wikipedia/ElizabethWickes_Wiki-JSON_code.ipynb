{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON and CSV\n",
    "\n",
    "Last week we talked about the the dictionary structure in Python.  How it can be used to collect data in key/value pairs.  These pairs can be quite valuable, particularly when you have many things to count.  So instead of having many separate accumulator structures, you can neatly store them all in a dictionary and let it automatically grow to hold more things as you are going over your content.\n",
    "\n",
    "Now we're going to go through and try to discover looking at it in context along with how these can be used with CSVs.  \n",
    "\n",
    "It is often the case that you are given a dataset in JSON format, but you need to get it into a CSV or rectangular format for analysis.  This is what you'll be doing for your homework, and we'll be walking through a few key points about each along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import statistics\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_search_url(term, offset):\n",
    "    left = \"https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch=\"\n",
    "    right = \"&srwhat=text&srlimit=500&format=json\"\n",
    "    sanitize = \"%20\".join(term.split())\n",
    "    offsetchunk = \"&sroffset=\" + str(offset) \n",
    "    return left + sanitize + right + offsetchunk\n",
    "\n",
    "def get_pages(term):\n",
    "    results = []\n",
    "    offset = 0\n",
    "    cont = True\n",
    "    while cont:\n",
    "        url = create_search_url(term, offset)\n",
    "        print(url)\n",
    "        r = requests.get(url)\n",
    "        datadump = json.loads(r.text)\n",
    "\n",
    "        if 'continue' in datadump:\n",
    "            results.append(datadump['query']['search'])\n",
    "            offset = datadump['continue']['sroffset']\n",
    "            time.sleep(3)\n",
    "        else:\n",
    "            results.append(datadump['query']['search'])\n",
    "            time.sleep(3)\n",
    "            cont = False \n",
    "    return results\n",
    "            \n",
    "def get_clean_snippets(chunks):\n",
    "    snippets = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        subset = [BeautifulSoup(d['snippet'], \"lxml\").text for d in chunk]\n",
    "        snippets += subset\n",
    "    \n",
    "    return snippets\n",
    "\n",
    "def get_sentiment_stats(snippets):\n",
    "    senti = []\n",
    "\n",
    "    for s in snippets:\n",
    "        blob = TextBlob(s, analyzer = NaiveBayesAnalyzer())\n",
    "        senti.append(blob.sentiment.polarity)\n",
    "    \n",
    "    return statistics.mean(senti), statistics.stdev(senti)\n",
    "\n",
    "def write_results(pageresults, filename):\n",
    "    j = []\n",
    "\n",
    "    for chunk in pageresults:\n",
    "        for d in chunk:\n",
    "            j.append(d)\n",
    "\n",
    "    with open(filename, 'w') as fout:\n",
    "        json.dump(j, fout, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch=cocker%20spaniel%20AND%20dog&srwhat=text&srlimit=500&format=json&sroffset=0\n"
     ]
    }
   ],
   "source": [
    "pages = get_pages(\"cocker spaniel AND dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337\n"
     ]
    }
   ],
   "source": [
    "print(len(get_clean_snippets(pages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_results(pages, 'cockerspanielresults.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'j' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ca83e63748f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'alljsonresults.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'j' is not defined"
     ]
    }
   ],
   "source": [
    "with open('alljsonresults.json', 'w') as fout:\n",
    "    json.dump(j, fout, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "senti = []\n",
    "\n",
    "for s in snippets:\n",
    "    blob = TextBlob(s)\n",
    "    senti.append(blob.sentiment.polarity)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(statistics.stdev(senti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(statistics.mean(senti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pitbull = get_clean_snippets(get_pages(\"husky\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pbmean, pbsd = get_sentiment_stats(pitbull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pbmean, pbsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corgi = get_clean_snippets(get_pages(\"cocker spaniel AND dog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shibamean, shibasd = get_sentiment_stats(get_clean_snippets(pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shibamean, shibasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter([1,2,3], [4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senti = []\n",
    "\n",
    "for s in get_clean_snippets(pages):\n",
    "    blob = TextBlob(s, analyzer = NaiveBayesAnalyzer())\n",
    "    pos = blob.sentiment.p_pos\n",
    "    print(pos, blob)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senti[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos = [s[0] for s in senti]\n",
    "neg = [s[1] for s in senti]\n",
    "\n",
    "plt.scatter(pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
